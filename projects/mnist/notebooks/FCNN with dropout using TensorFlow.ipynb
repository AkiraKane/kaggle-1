{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and assign to X and y\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "X_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "y_train = train[\"label\"]\n",
    "\n",
    "# Now normalize the test and train sets\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Reshape image in 3 dimensions (height = 28px, width = 28px)\n",
    "X_train = X_train.values.reshape(-1,28,28)\n",
    "X_test = X_test.values.reshape(-1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 42000, test size: 28000\n"
     ]
    }
   ],
   "source": [
    "# What are the sizes of the train and test sets?\n",
    "print('train size: ' + str(len(X_train)) + ', test size: ' + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# So we have 42,000 train images and 28,000 test images\n",
    "# let's go ahead and print one\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x119385f60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's now reshape it into a 28 x 28 matrix\n",
    "image = X_train[9]\n",
    "print(image.shape)\n",
    "\n",
    "# Let's plot it now\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [7],\n",
       "       [6],\n",
       "       [9]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's look at the labels\n",
    "print(y_train.values.shape)\n",
    "y_train.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the labels are NOT one hot encoded. We need to do that\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train_ohe = enc.fit_transform(y_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first reset our graph, so our neural network components are all declared within the same graph\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [784, None])\n",
    "Y = tf.placeholder(tf.float32, [10, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weights for the input layer\n",
    "W1 = tf.get_variable(\"W1\", [64, 784], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "# The bias for the output from the input layer\n",
    "b1 = tf.get_variable(\"b1\", [64, 1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer (input layer)\n",
    "Z1 = tf.add(tf.matmul(W1, X), b1)\n",
    "\n",
    "# Let's add the activation function to the output signal from the first layer\n",
    "A1 = tf.nn.relu(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W2 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-35-595d4d458241>\", line 1, in <module>\n    W2 = tf.get_variable(\"W2\", [20, 64], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-df1da0edd548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Weights and bias for the hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1299\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    437\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 747\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W2 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-35-595d4d458241>\", line 1, in <module>\n    W2 = tf.get_variable(\"W2\", [20, 64], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# Weights and bias for the hidden layer\n",
    "W2 = tf.get_variable(\"W2\", [20, 64], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "b2 = tf.get_variable(\"b2\", [20, 1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second layer (hidden layer)\n",
    "Z2 = tf.add(tf.matmul(W2, A1), b2) \n",
    "\n",
    "# Let's add the activation function to the output signal from the second layer\n",
    "A2 = tf.nn.relu(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and bias for the output layer\n",
    "W3 = tf.get_variable(\"W3\", [10, 20], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "b3 = tf.get_variable(\"b3\", [10, 1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third layer (output layer)\n",
    "Z3 = tf.add(tf.matmul(W3, A2), b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(Z3), labels=tf.transpose(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate for Gradient Descent algorithm\n",
    "learning_rate = 0.5\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to run the graph\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 164\n"
     ]
    }
   ],
   "source": [
    "epochs = 20                                    # run a 20 epochs\n",
    "batch_size = 256                               # for each epoch, train in batches of 200 images\n",
    "number_of_images = X_train.shape[0] # number of images in training data\n",
    "batches = number_of_images // batch_size       # number of batches in an epoch\n",
    "print(\"Number of batches:\", batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # run our training data through the neural network for each epoch\n",
    "    for epoch in range(epochs):\n",
    "      # Get a batch (random shuffled) from the training data\n",
    "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "      \n",
    "      # Feed this batch through the neural network.\n",
    "      _, epoch_cost = sess.run([optimizer, cost], feed_dict={X: batch_xs.T, Y: batch_ys.T})\n",
    "      \n",
    "      if epoch % 100 == 0:\n",
    "        print(\"Epoch: \", epoch, epoch_cost)\n",
    "        \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Training Time:\", end - start)\n",
    "    \n",
    "    # Test the Model\n",
    "    \n",
    "    # Let's select the highest percent from the softmax output per image as the prediction.\n",
    "    prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "    \n",
    "    # Let's create another node for calculating the accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "    # Now let's run our trainingt images through the model to calculate our accuracy during training\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: mnist.train.images.T, Y: mnist.train.labels.T}))\n",
    "    \n",
    "    # Now let's run our test images through the model to calculate our accuracy on the test data\n",
    "    print (\"Test Accuracy:\", accuracy.eval({X: mnist.test.images.T, Y: mnist.test.labels.T}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
